+++
title = 'Kaggle: House Price Predicting'
date = 2024-06-26T19:27:00+08:00
draft = false
tags = ["project", "xmu", "2024"]
+++

### Import libraries

```python
# import libraries
import numpy as np
import pandas as pd
import torch
from torch import nn
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader, TensorDataset
```

### Ingest data

Raw data can be downloaded here.

- [train.csv](/raw_data/xmu/240626/train.csv)
- [test.csv](/raw_data/xmu/240626/test.csv)

```python
# ingest data
train_data = pd.read_csv("house-price-data/train.csv")
test_data = pd.read_csv("house-price-data/test.csv")
```

### Data preprocessing

We preprocess the data set first.

- Numerical data **standardization**

- Fill in or remove **missing values**

Filter the features whose type is non-object in "all_features" (that is, numerical features) and standardize them

$$
x\leftarrow \frac{x-\mu(x)}{\sigma(x)}
$$

For features of type "object", convert them to dummy variables (pd.get_dummies) to make them numeric. "Dummy_na=True" treats "na" (missing value) as a valid feature value and creates an indicator feature for it.

```python
# data preprocessing
all_features = pd.concat((train_data.iloc[:, 1:-1], test_data.iloc[:, 1:]))

# standardizing numerical data
numeric_features = all_features.dtypes[all_features.dtypes != 'object'].index
all_features[numeric_features] = all_features[numeric_features].apply(
    lambda x: (x - x.mean()) / (x.std()))

# convert categoric to numeric
all_features = pd.get_dummies(all_features, dummy_na=True)

# delete missing data
all_features.dropna(axis=1, how="any", inplace=True)

# split train and test sets
n_train = train_data.shape[0]
train_features = torch.tensor(all_features[:n_train].values.tolist(), dtype=torch.float32)
test_features = torch.tensor(all_features[n_train:].values.tolist(), dtype=torch.float32)
train_labels = torch.tensor(
    train_data.SalePrice.values.reshape(-1, 1), dtype=torch.float32)
```

### Train the model

For house prices, we are more concerned with relative error

$$
\frac{y-\hat{y}}{y}
$$

Instead of absolute error \(y-\hat{y}\)

Therefore, the difference is usually measured as the logarithm of the price forecast. This is also the error indicator used by competition officials to evaluate the quality of model submissions. The formula is derived as follows:

\(\exists \delta>0\), s.t. \(|\log y-\log\hat{y}|\leqslant \delta\), namely

$$
-\delta\leqslant \log \frac{y}{\hat{y}}\leqslant \delta
$$

$$
e^{-\delta} \leq \frac{\hat{y}}{y} \leq e^\delta
$$

This gives the following root-mean-square error between the logarithm of the predicted price and the logarithm of the true label price

$$
\sqrt{\frac{1}{n}\sum_{i=1}^n\left(\log y_i -\log \hat{y}_i\right)^2}.
$$

```python
def plot(X, Y=None, xlabel=None, ylabel=None, legend=[], xlim=None, ylim=None, xscale='linear', yscale='linear', fmts=('-', 'c--', 'b-.', 'orange:'), figsize=(3.5, 2.5), axes=None):
    """
    Visualization
    """
    def has_one_axis(X):
        """
        Determines if X has only one axis
        """
        return (hasattr(X, "ndim") and X.ndim == 1 or isinstance(X, list)
                and not hasattr(X[0], "__len__"))

    if has_one_axis(X):  # if X is 1D, wrap in list
        X = [X]
    if Y is None:  # if Y not provided, assume Y = X and X is empty
        X, Y = [[]] * len(X), X
    elif has_one_axis(Y):  # if Y is 1D, wrap in list
        Y = [Y]
    if len(X) != len(Y):  # ensure X, Y have same length
        X = X * len(Y)

    plt.figure(figsize=figsize, dpi=200)
    if axes is None:
        axes = plt.gca()
    axes.cla()  # clear axes

    # plot each series using the given formats
    for x, y, fmt in zip(X, Y, fmts):
        axes.plot(x,y,fmt) if len(x) else axes.plot(y,fmt)
    axes.set_xlabel(xlabel), axes.set_ylabel(ylabel)
    axes.set_xscale(xscale), axes.set_yscale(yscale)
    axes.set_xlim(xlim),     axes.set_ylim(ylim)
    if legend:
        axes.legend(legend)
    axes.grid()
```

```python
class NetTrainer:
    """
    Contruct MLP model
    """
    def __init__(self, input_size, hidden_sizes=[128, 64, 32]):
        self.loss = nn.MSELoss()
        self.input_size = input_size
        self.hidden_sizes = hidden_sizes
        self.model = self.build_model()

    def build_model(self):
        layers = []
        in_size = self.input_size
        for hidden_size in self.hidden_sizes:
            layers.append(nn.Linear(in_size, hidden_size))
            layers.append(nn.ReLU())
            in_size = hidden_size
        layers.append(nn.Linear(in_size, 1))
        return nn.Sequential(*layers)

    def log_rmse(self, features, targets):
        predictions = torch.clamp(self.model(features), min=1)  # prevent calculation errors
        rmse = torch.sqrt(self.loss(torch.log(predictions), torch.log(targets)))
        return rmse.item()

    def data_loader(self, data, batch_size, is_training=True):
        """
        Pytorch iterator
        """
        dataset = TensorDataset(*data)
        # split by batch to build iterators
        return DataLoader(dataset, batch_size, shuffle=is_training)

    def train_model(self, train_features, train_labels, valid_features, valid_labels, epochs, lr, weight_decay, batch_size):
        train_losses, val_losses = [], []
        # load iterator
        train_loader = self.data_loader((train_features, train_labels), batch_size)
        # choose Adam optimizer
        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr, weight_decay=weight_decay)
        for epoch in range(epochs):
            for X_batch, y_batch in train_loader:
                optimizer.zero_grad()
                loss = self.loss(self.model(X_batch), y_batch)
                loss.backward()
                optimizer.step()
            train_losses.append(self.log_rmse(train_features, train_labels))
            if valid_labels is not None:
                val_losses.append(self.log_rmse(valid_features, valid_labels))
        return train_losses, val_losses

    def split_kfold_data(self, X, y, k, i):
        """
        In each loop, extract a fold as validation set, leave the rest as training set
        """
        assert k > 1
        fold_size = X.shape[0] // k
        X_train, y_train = None, None
        for j in range(k):
            idx = slice(j * fold_size, (j + 1) * fold_size)
            if j == i:
                X_valid, y_valid = X[idx, :], y[idx]
            elif X_train is None:
                X_train, y_train = X[idx, :], y[idx]
            else:
                X_train = torch.cat([X_train, X[idx, :]], axis=0)
                y_train = torch.cat([y_train, y[idx]], axis=0)
        return X_train, y_train, X_valid, y_valid

    def k_fold_cross_validation(self, k, X, y, epochs, lr, weight_decay, batch_size):
        """
        kfold validate model performance
        """
        total_train_loss, total_val_loss = 0, 0
        for i in range(k):
            # split training set and validation set
            X_train, y_train, X_val, y_val = self.split_kfold_data(X, y, k, i)
            self.model = self.build_model()
            train_losses, val_losses = self.train_model(X_train, y_train, X_val, y_val, epochs, lr, weight_decay, batch_size)
            total_train_loss += train_losses[-1]
            total_val_loss += val_losses[-1]
            if i == 0:
                # plot the training process
                plot(list(range(1, epochs + 1)), [train_losses, val_losses], xlabel='Epoch', ylabel='RMSE', xlim=[1, epochs], legend=['Train', 'Validation'], yscale='log')
            print(f'Fold {i+1}: Train loss = {train_losses[-1]:.4f}, Validation loss = {val_losses[-1]:.4f}')
        return total_train_loss / k, total_val_loss / k
    
    def train_and_predict(self, train_features, test_features, train_labels, test_df, epochs, lr, weight_decay, batch_size):
        """
        model training and predict
        """
        self.model = self.build_model()
        # train the model
        train_losses, _ = self.train_model(train_features, train_labels, None, None, epochs, lr, weight_decay, batch_size)
        plot(np.arange(1, epochs + 1), [train_losses], xlabel='Epoch', ylabel='Log RMSE', xlim=[1, epochs], yscale='log')
        print(f'Training Log RMSE: {float(train_losses[-1]):f}')
        predictions = self.model(test_features).detach().numpy()
        test_df['SalePrice'] = pd.Series(predictions.reshape(-1))
        submission = pd.concat([test_df['Id'], test_df['SalePrice']], axis=1)
        submission.to_csv('submission.csv', index=False)
```

### Visualization

```python
# set hyperparameters
k, num_epochs, learning_rate, weight_decay, batch_size = 5, 100, 0.01, 2, 64

# train
trainer = NetTrainer(train_features.shape[1])
avg_train_loss, avg_val_loss = trainer.k_fold_cross_validation(k, train_features, train_labels, num_epochs, learning_rate, weight_decay, batch_size)

print(f'{k}-Fold Validation: Average Training Log RMSE: {float(avg_train_loss):.4f}, Average Validation Log RMSE: {float(avg_val_loss):.4f}')
```

![output1](/img/xmu/240626/output1.png)

When training data is scarce, we may not even be able to provide enough data to form a suitable validation set. A popular solution to this problem is to employ K-fold cross-validation. Here, the raw training data is divided into K non-overlapping subsets. The model is then trained and validated K times, each time on K-1 subsets, and validated on the remaining subset (the subset that was not used for training in that round). Finally, the training and validation errors are estimated by **averaging** the results of K experiments.

As can be seen from the loss function plot above, the model bifurcates around the 10th epoch, the training error continues to decline, while the validation error begins to rise, indicating gradual overfitting. The best number of iterations should be at epoch=10.

```python
num_epochs = 20
_, _ = trainer.k_fold_cross_validation(k, train_features, train_labels, 20, learning_rate, weight_decay, batch_size)
```

![output2](/img/xmu/240626/output2.png)

```python
trainer.train_and_predict(train_features, test_features, train_labels, test_data, num_epochs, learning_rate, weight_decay, batch_size)
```

![output3](/img/xmu/240626/output3.png)

```python
# adjust hyperparameters
num_epochs = 100

# train
trainer = NetTrainer(train_features.shape[1], hidden_sizes=[128, 64, 32])
avg_train_loss, avg_val_loss = trainer.k_fold_cross_validation(k, train_features, train_labels, num_epochs, learning_rate, weight_decay, batch_size)

print(f'{k}-Fold Validation: Average Training Log RMSE: {float(avg_train_loss):.4f}, Average Validation Log RMSE: {float(avg_val_loss):.4f}')
```

![output4](/img/xmu/240626/output4.png)

```python
trainer.train_and_predict(train_features, test_features, train_labels, test_data, num_epochs, learning_rate, weight_decay, batch_size)
```

![output5](/img/xmu/240626/output5.png)

We evaluated the model using 50-fold cross-validation. Throughout the training process, the average training Log RMSE was 0.0807 and the average verification Log RMSE was 0.1592. This shows that our model has good fitting ability on both the training set and the verification set, and the verification error is higher than the training error, indicating that the model is slightly overfitting.

In each fold, the trend of training and validation losses is consistent, which indicates that the performance of the model is stable across different data sets. However, in some folds, we observe large fluctuations in validation losses, which may be due to the presence of noisy data on the validation set or the sensitivity of the model to some specific features.

By plotting a curve for training and verifying losses, we find that:

- In the early stages of training, losses dropped rapidly, indicating that the model learned the main features of the data relatively quickly.
- As the training progresses, the downward trend of loss gradually flattens out, which indicates that the model begins to converge.
- In some folds, validation losses rose slightly later in the training, suggesting possible overfitting.

Although the model performs well on both the training set and the verification set, the loss on the verification set is slightly higher than that on the training set, suggesting that the model is slightly overfitting. This means that the model fits the training data well, but underperforms on new data.